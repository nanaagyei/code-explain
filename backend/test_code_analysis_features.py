"""
Comprehensive tests for AI-powered code analysis features.

Tests:
- Code Review (security, performance, best practices)
- Quality Metrics (5-dimensional scoring)
- Architecture Diagrams (component extraction)
- Mentor Insights (skill assessment & learning paths)

Run with: python test_code_analysis_features.py
"""
import asyncio
import json
import time
import requests
from typing import Dict, List, Any

# Test configuration
BASE_URL = "http://localhost:8000"
TEST_USER = {
    "username": f"testuser_{int(time.time())}",
    "email": f"testuser_{int(time.time())}@example.com",
    "password": "testpass123"
}

# Sample code for testing different features
SAMPLE_CODE = {
    "python_simple": '''
def calculate_fibonacci(n):
    """Calculate Fibonacci number recursively."""
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)

class DataProcessor:
    def __init__(self, data):
        self.data = data
        self.secret_key = "hardcoded_secret_123"  # Security issue
    
    def process(self):
        # Performance issue: O(n¬≤) algorithm
        result = []
        for item in self.data:
            for other in self.data:
                if item == other:
                    result.append(item * 2)
        return result
''',
    "javascript_complex": '''
class UserManager {
    constructor() {
        this.users = [];
        this.adminPassword = "admin123";  // Security issue
    }
    
    async fetchUsers() {
        // Performance issue: sequential API calls
        const users = [];
        for (let id = 1; id <= 100; id++) {
            const response = await fetch(`/api/users/${id}`);
            users.push(await response.json());
        }
        return users;
    }
    
    validateUser(user) {
        // Best practice issue: no input validation
        return user.name && user.email;
    }
}

function processData(data) {
    // Security issue: eval usage
    return eval(data.expression);
}
'''
}

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'

def print_header(text: str):
    print(f"\n{Colors.BOLD}{Colors.HEADER}{'='*70}{Colors.END}")
    print(f"{Colors.BOLD}{Colors.HEADER}{text.center(70)}{Colors.END}")
    print(f"{Colors.BOLD}{Colors.HEADER}{'='*70}{Colors.END}\n")

def print_success(text: str):
    print(f"{Colors.GREEN}‚úì {text}{Colors.END}")

def print_error(text: str):
    print(f"{Colors.RED}‚úó {text}{Colors.END}")

def print_info(text: str):
    print(f"{Colors.CYAN}‚Ñπ {text}{Colors.END}")

def print_step(step: str):
    print(f"\n{Colors.BOLD}{Colors.BLUE}{step}{Colors.END}")

class CodeAnalysisTester:
    def __init__(self):
        self.token = None
        self.headers = None
        self.repo_id = None
        self.file_id = None
        
    async def setup_user_and_repo(self):
        """Setup test user and repository"""
        print_step("1Ô∏è‚É£  Setting up test user and repository")
        
        # Register user
        register_response = requests.post(f"{BASE_URL}/auth/register", json=TEST_USER)
        if register_response.status_code not in [200, 201]:
            print_error(f"User registration failed: {register_response.status_code}")
            return False
        
        print_success(f"User registered: {TEST_USER['username']}")
        
        # Login
        login_response = requests.post(
            f"{BASE_URL}/auth/login",
            data={"username": TEST_USER["username"], "password": TEST_USER["password"]}
        )
        
        if login_response.status_code != 200:
            print_error(f"Login failed: {login_response.status_code}")
            return False
        
        self.token = login_response.json()["access_token"]
        self.headers = {"Authorization": f"Bearer {self.token}"}
        print_success("Login successful")
        
        # Upload test code
        files = {
            'files': ('test_code.py', SAMPLE_CODE["python_simple"], 'text/plain')
        }
        data = {'name': 'Code Analysis Test Repo'}
        
        upload_response = requests.post(
            f"{BASE_URL}/repositories/",
            headers=self.headers,
            files=files,
            data=data
        )
        
        if upload_response.status_code != 201:
            print_error(f"Repository upload failed: {upload_response.status_code}")
            return False
        
        self.repo_id = upload_response.json()["id"]
        print_success(f"Repository created: ID {self.repo_id}")
        
        # Wait for processing
        print_info("Waiting for file processing...")
        for i in range(30):  # Wait up to 60 seconds
            time.sleep(2)
            response = requests.get(f"{BASE_URL}/repositories/{self.repo_id}", headers=self.headers)
            if response.status_code == 200:
                response_data = response.json()
                repo_data = response_data["repository"]
                files_data = response_data.get("files", [])
                
                if repo_data["status"] == "completed":
                    if files_data and len(files_data) > 0:
                        self.file_id = files_data[0]["id"]
                        print_success("File processing completed")
                        return True
                    else:
                        print_error("No files found in repository")
                        return False
                elif repo_data["status"] == "failed":
                    print_error("File processing failed")
                    return False
        
        print_error("Timeout waiting for file processing")
        return False
    
    async def test_code_review(self):
        """Test AI code review functionality"""
        print_step("2Ô∏è‚É£  Testing AI Code Review")
        
        if not self.repo_id or not self.file_id:
            print_error("No repository or file available for testing")
            return False
        
        # Generate code review
        review_response = requests.post(
            f"{BASE_URL}/code-analysis/repositories/{self.repo_id}/files/{self.file_id}/review",
            headers=self.headers
        )
        
        if review_response.status_code != 200:
            print_error(f"Code review failed: {review_response.status_code}")
            print_info(f"Response: {review_response.text}")
            return False
        
        response_data = review_response.json()
        print_success("Code review generated successfully")
        
        # Extract the actual code review data from the response wrapper
        if "code_review" not in response_data:
            print_error("Missing 'code_review' field in response")
            return False
        
        review_data = response_data["code_review"]
        
        # Validate response structure
        required_fields = ["security_issues", "performance_issues", "best_practices", "overall_score", "summary"]
        for field in required_fields:
            if field not in review_data:
                print_error(f"Missing required field: {field}")
                return False
        
        print_success("Response structure validated")
        
        # Display results
        print_info(f"Overall Score: {review_data['overall_score']}/100")
        print_info(f"Security Issues: {len(review_data['security_issues'])}")
        print_info(f"Performance Issues: {len(review_data['performance_issues'])}")
        print_info(f"Best Practices: {len(review_data['best_practices'])}")
        
        # Validate that security issues were detected
        if len(review_data['security_issues']) > 0:
            print_success("Security vulnerabilities detected (expected)")
            for issue in review_data['security_issues'][:2]:  # Show first 2
                print_info(f"  - {issue['type']}: {issue['description']}")
        
        # Validate that performance issues were detected
        if len(review_data['performance_issues']) > 0:
            print_success("Performance issues detected (expected)")
            for issue in review_data['performance_issues'][:2]:  # Show first 2
                print_info(f"  - {issue['type']}: {issue['description']}")
        
        return True
    
    async def test_quality_metrics(self):
        """Test quality metrics calculation"""
        print_step("3Ô∏è‚É£  Testing Quality Metrics")
        
        if not self.repo_id or not self.file_id:
            print_error("No repository or file available for testing")
            return False
        
        # Calculate quality metrics
        metrics_response = requests.post(
            f"{BASE_URL}/code-analysis/repositories/{self.repo_id}/files/{self.file_id}/quality",
            headers=self.headers
        )
        
        if metrics_response.status_code != 200:
            print_error(f"Quality metrics failed: {metrics_response.status_code}")
            return False
        
        response_data = metrics_response.json()
        print_success("Quality metrics calculated successfully")
        
        # Extract the actual quality metrics data from the response wrapper
        if "quality_metrics" not in response_data:
            print_error("Missing 'quality_metrics' field in response")
            return False
        
        metrics_data = response_data["quality_metrics"]
        
        # Validate response structure
        required_fields = ["maintainability", "testability", "readability", "performance", "security", "overall", "breakdown"]
        for field in required_fields:
            if field not in metrics_data:
                print_error(f"Missing required field: {field}")
                return False
        
        print_success("Response structure validated")
        
        # Display results
        metrics = ["maintainability", "testability", "readability", "performance", "security"]
        for metric in metrics:
            score = metrics_data[metric]
            print_info(f"{metric.capitalize()}: {score}/100")
        
        print_info(f"Overall Score: {metrics_data['overall']}/100")
        
        # Validate score ranges
        for metric in metrics + ["overall"]:
            score = metrics_data[metric]
            if not isinstance(score, (int, float)) or score < 0 or score > 100:
                print_error(f"Invalid score for {metric}: {score}")
                return False
        
        print_success("All scores within valid range (0-100)")
        
        return True
    
    async def test_architecture_diagram(self):
        """Test architecture diagram generation"""
        print_step("4Ô∏è‚É£  Testing Architecture Diagram")
        
        if not self.repo_id or not self.file_id:
            print_error("No repository or file available for testing")
            return False
        
        # Generate architecture diagram
        diagram_response = requests.post(
            f"{BASE_URL}/code-analysis/repositories/{self.repo_id}/files/{self.file_id}/architecture",
            headers=self.headers
        )
        
        if diagram_response.status_code != 200:
            print_error(f"Architecture diagram failed: {diagram_response.status_code}")
            return False
        
        response_data = diagram_response.json()
        print_success("Architecture diagram generated successfully")
        
        # Extract the actual architecture diagram data from the response wrapper
        if "architecture_diagram" not in response_data:
            print_error("Missing 'architecture_diagram' field in response")
            return False
        
        diagram_data = response_data["architecture_diagram"]
        
        # Validate response structure
        required_fields = ["nodes", "edges", "layout"]
        for field in required_fields:
            if field not in diagram_data:
                print_error(f"Missing required field: {field}")
                return False
        
        print_success("Response structure validated")
        
        # Display results
        print_info(f"Nodes: {len(diagram_data['nodes'])}")
        print_info(f"Edges: {len(diagram_data['edges'])}")
        print_info(f"Layout: {diagram_data['layout']}")
        
        # Validate node structure
        if diagram_data['nodes']:
            node = diagram_data['nodes'][0]
            node_fields = ["id", "type", "label", "description"]
            for field in node_fields:
                if field not in node:
                    print_error(f"Missing node field: {field}")
                    return False
            
            print_success("Node structure validated")
            print_info(f"  Sample node: {node['label']} ({node['type']})")
        
        # Validate edge structure
        if diagram_data['edges']:
            edge = diagram_data['edges'][0]
            edge_fields = ["id", "source", "target", "label", "type"]
            for field in edge_fields:
                if field not in edge:
                    print_error(f"Missing edge field: {field}")
                    return False
            
            print_success("Edge structure validated")
        
        return True
    
    async def test_mentor_insights(self):
        """Test mentor insights generation"""
        print_step("5Ô∏è‚É£  Testing Mentor Insights")
        
        if not self.repo_id or not self.file_id:
            print_error("No repository or file available for testing")
            return False
        
        # Generate mentor insights
        mentor_response = requests.post(
            f"{BASE_URL}/code-analysis/repositories/{self.repo_id}/files/{self.file_id}/mentor",
            headers=self.headers
        )
        
        if mentor_response.status_code != 200:
            print_error(f"Mentor insights failed: {mentor_response.status_code}")
            return False
        
        response_data = mentor_response.json()
        print_success("Mentor insights generated successfully")
        
        # Extract the actual mentor insights data from the response wrapper
        if "mentor_insights" not in response_data:
            print_error("Missing 'mentor_insights' field in response")
            return False
        
        mentor_data = response_data["mentor_insights"]
        
        # Validate response structure
        required_fields = ["skill_level", "strengths", "weaknesses", "learning_path", "challenges", "estimated_time", "next_milestone"]
        for field in required_fields:
            if field not in mentor_data:
                print_error(f"Missing required field: {field}")
                return False
        
        print_success("Response structure validated")
        
        # Display results
        print_info(f"Skill Level: {mentor_data['skill_level']}")
        print_info(f"Strengths: {len(mentor_data['strengths'])} identified")
        print_info(f"Weaknesses: {len(mentor_data['weaknesses'])} identified")
        print_info(f"Learning Path: {len(mentor_data['learning_path'])} modules")
        print_info(f"Challenges: {len(mentor_data['challenges'])} exercises")
        print_info(f"Estimated Time: {mentor_data['estimated_time']}")
        
        # Validate skill level
        valid_levels = ["beginner", "intermediate", "advanced"]
        if mentor_data['skill_level'] not in valid_levels:
            print_error(f"Invalid skill level: {mentor_data['skill_level']}")
            return False
        
        print_success("Skill level validation passed")
        
        # Show sample learning path item
        if mentor_data['learning_path']:
            item = mentor_data['learning_path'][0]
            print_info(f"  Sample learning: {item.get('title', 'N/A')}")
        
        return True
    
    async def test_batch_analysis(self):
        """Test batch analysis functionality"""
        print_step("6Ô∏è‚É£  Testing Batch Analysis")
        
        if not self.repo_id:
            print_error("No repository available for testing")
            return False
        
        # Request batch analysis
        batch_request = {
            "analysis_types": ["code_review", "quality_metrics", "architecture_diagram", "mentor_insights"],
            "include_summary": True
        }
        
        batch_response = requests.post(
            f"{BASE_URL}/code-analysis/repositories/{self.repo_id}/analyze-all",
            headers=self.headers,
            json=batch_request
        )
        
        if batch_response.status_code != 200:
            print_error(f"Batch analysis failed: {batch_response.status_code}")
            return False
        
        batch_data = batch_response.json()
        print_success("Batch analysis completed successfully")
        
        # Validate response structure
        required_fields = ["results", "processing_time", "cached_counts"]
        for field in required_fields:
            if field not in batch_data:
                print_error(f"Missing required field: {field}")
                return False
        
        print_success("Response structure validated")
        
        # Display results
        print_info(f"Processing Time: {batch_data['processing_time']:.2f}s")
        print_info(f"Cached Results: {batch_data['cached_counts']}")
        print_info(f"Analysis Types: {list(batch_data['results'].keys())}")
        
        return True
    
    async def run_all_tests(self):
        """Run all code analysis tests"""
        print_header("AI Code Analysis Features Test Suite")
        
        # Setup
        if not await self.setup_user_and_repo():
            print_error("Setup failed, cannot continue with tests")
            return False
        
        # Run individual tests
        tests = [
            self.test_code_review,
            self.test_quality_metrics,
            self.test_architecture_diagram,
            self.test_mentor_insights,
            self.test_batch_analysis
        ]
        
        passed = 0
        total = len(tests)
        
        for test in tests:
            try:
                if await test():
                    passed += 1
                else:
                    print_error(f"Test failed: {test.__name__}")
            except Exception as e:
                print_error(f"Test error in {test.__name__}: {e}")
        
        # Final report
        print_header("Test Results Summary")
        
        if passed == total:
            print_success(f"All {total} tests passed! üéâ")
            
            print(f"\n{Colors.BOLD}Features Verified:{Colors.END}")
            print_success("AI Code Review (security, performance, best practices)")
            print_success("Quality Metrics (5-dimensional scoring)")
            print_success("Architecture Diagrams (component extraction)")
            print_success("Mentor Insights (skill assessment & learning paths)")
            print_success("Batch Analysis (multiple analyses at once)")
            
            print(f"\n{Colors.BOLD}Technical Validation:{Colors.END}")
            print_success("API endpoints responding correctly")
            print_success("JSON response structure validation")
            print_success("Error handling and edge cases")
            print_success("Performance and caching")
            
            return True
        else:
            print_error(f"Only {passed}/{total} tests passed")
            return False

async def main():
    """Main test runner"""
    tester = CodeAnalysisTester()
    
    try:
        success = await tester.run_all_tests()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print_info("\n\nTest interrupted by user")
        exit(1)
    except Exception as e:
        print_error(f"\n\nUnexpected error: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
